{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ],
      "metadata": {
        "id": "CtuSrazlNYEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ],
      "metadata": {
        "id": "vriXNd_nL2q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Combine all sonnets into a single text source.  \n",
        "- Split into training (80%) and validation (20%).  "
      ],
      "metadata": {
        "id": "Q5atve1sMH9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O book.txt https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
        "\n",
        "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Loaded text length:\", len(text))\n",
        "\n",
        "split_index = int(len(text) * 0.8)\n",
        "train_text = text[:split_index]\n",
        "test_text = text[split_index:]\n",
        "\n",
        "print(\"Train text length:\", len(train_text))\n",
        "print(\"Test text length:\", len(test_text))"
      ],
      "metadata": {
        "id": "QvKdt5EyMDug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e56a49-3251-48a7-e82b-d299d3aa2e9a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-20 10:06:45--  https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 306594 (299K) [text/plain]\n",
            "Saving to: ‘book.txt’\n",
            "\n",
            "book.txt            100%[===================>] 299.41K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-20 10:06:45 (2.50 MB/s) - ‘book.txt’ saved [306594/306594]\n",
            "\n",
            "Loaded text length: 290077\n",
            "Train text length: 232061\n",
            "Test text length: 58016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ],
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def preprocess(raw_text):\n",
        "    raw_text = raw_text.lower()\n",
        "    allowed_punctuation = ['.', '!', '?']\n",
        "    clean_text = \"\"\n",
        "    for c in raw_text:\n",
        "        if c.isalnum() or c.isspace() or c in allowed_punctuation:\n",
        "            clean_text += c\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "train_text = preprocess(train_text)\n",
        "test_text = preprocess(test_text)\n",
        "\n",
        "\n",
        "chars = sorted(set(train_text))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "char2id = {ch: idx for idx, ch in enumerate(chars)}\n",
        "id2char = {idx: ch for ch, idx in char2id.items()}\n",
        "\n",
        "train_ids = [char2id[c] for c in train_text]\n",
        "test_ids = [char2id.get(c, char2id[' ']) for c in test_text]\n",
        "\n",
        "seq_length = 100\n",
        "\n",
        "def create_sequences(token_ids, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(token_ids) - seq_length):\n",
        "        X.append(token_ids[i:i + seq_length])\n",
        "        y.append(token_ids[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_train, y_train = create_sequences(train_ids, seq_length)\n",
        "X_test, y_test = create_sequences(test_ids, seq_length)\n",
        "\n",
        "print(\"Char vocab size:\", vocab_size)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "RvXRFVcbMLe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27520ae0-7c37-475f-ec41-57237fddbab2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char vocab size: 46\n",
            "X_train shape: (223547, 100)\n",
            "y_train shape: (223547,)\n",
            "X_test shape: (56059, 100)\n",
            "y_test shape: (56059,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ],
      "metadata": {
        "id": "jbTZs3OiMMNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=256, input_length=20),\n",
        "    GRU(128),\n",
        "    Dropout(0.2),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "OXCK40l6MRld"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ],
      "metadata": {
        "id": "qsXR4RZpMXMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "linweGaUMg0T"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible."
      ],
      "metadata": {
        "id": "Ggop4h4IMhMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "class PerplexityCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        train_loss = logs.get('loss')\n",
        "        val_loss = logs.get('val_loss')\n",
        "        if train_loss:\n",
        "            print(f\"Train Perplexity: {math.exp(train_loss):.2f}\")\n",
        "        if val_loss:\n",
        "            print(f\"Validation Perplexity: {math.exp(val_loss):.2f}\")\n",
        "\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[PerplexityCallback(), early_stop]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "P8d8FS2XMj46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fa39d3-def5-4013-ee8e-a66630e74b56"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 0.3104 - loss: 2.3880Train Perplexity: 8.50\n",
            "Validation Perplexity: 7.18\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 454ms/step - accuracy: 0.3104 - loss: 2.3878 - val_accuracy: 0.4213 - val_loss: 1.9712\n",
            "Epoch 2/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 0.4380 - loss: 1.8658Train Perplexity: 6.24\n",
            "Validation Perplexity: 6.38\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 454ms/step - accuracy: 0.4380 - loss: 1.8657 - val_accuracy: 0.4481 - val_loss: 1.8529\n",
            "Epoch 3/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.4693 - loss: 1.7479Train Perplexity: 5.66\n",
            "Validation Perplexity: 5.97\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m790s\u001b[0m 448ms/step - accuracy: 0.4693 - loss: 1.7479 - val_accuracy: 0.4666 - val_loss: 1.7871\n",
            "Epoch 4/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - accuracy: 0.4910 - loss: 1.6831Train Perplexity: 5.34\n",
            "Validation Perplexity: 5.78\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m779s\u001b[0m 446ms/step - accuracy: 0.4910 - loss: 1.6831 - val_accuracy: 0.4767 - val_loss: 1.7548\n",
            "Epoch 5/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.5005 - loss: 1.6449Train Perplexity: 5.13\n",
            "Validation Perplexity: 5.62\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m799s\u001b[0m 445ms/step - accuracy: 0.5005 - loss: 1.6448 - val_accuracy: 0.4853 - val_loss: 1.7257\n",
            "Epoch 6/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - accuracy: 0.5110 - loss: 1.6097Train Perplexity: 4.99\n",
            "Validation Perplexity: 5.52\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m803s\u001b[0m 445ms/step - accuracy: 0.5110 - loss: 1.6097 - val_accuracy: 0.4912 - val_loss: 1.7084\n",
            "Epoch 7/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.5198 - loss: 1.5784Train Perplexity: 4.88\n",
            "Validation Perplexity: 5.47\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m808s\u001b[0m 448ms/step - accuracy: 0.5198 - loss: 1.5784 - val_accuracy: 0.4925 - val_loss: 1.7001\n",
            "Epoch 8/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - accuracy: 0.5235 - loss: 1.5665Train Perplexity: 4.80\n",
            "Validation Perplexity: 5.43\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 428ms/step - accuracy: 0.5235 - loss: 1.5665 - val_accuracy: 0.4938 - val_loss: 1.6914\n",
            "Epoch 9/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.5280 - loss: 1.5460Train Perplexity: 4.72\n",
            "Validation Perplexity: 5.40\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m841s\u001b[0m 450ms/step - accuracy: 0.5280 - loss: 1.5460 - val_accuracy: 0.4996 - val_loss: 1.6865\n",
            "Epoch 10/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - accuracy: 0.5321 - loss: 1.5320Train Perplexity: 4.67\n",
            "Validation Perplexity: 5.36\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m811s\u001b[0m 456ms/step - accuracy: 0.5321 - loss: 1.5320 - val_accuracy: 0.5037 - val_loss: 1.6788\n",
            "Epoch 11/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - accuracy: 0.5354 - loss: 1.5191Train Perplexity: 4.61\n",
            "Validation Perplexity: 5.35\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 434ms/step - accuracy: 0.5354 - loss: 1.5191 - val_accuracy: 0.5017 - val_loss: 1.6768\n",
            "Epoch 12/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - accuracy: 0.5362 - loss: 1.5159Train Perplexity: 4.57\n",
            "Validation Perplexity: 5.30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 428ms/step - accuracy: 0.5362 - loss: 1.5159 - val_accuracy: 0.5027 - val_loss: 1.6686\n",
            "Epoch 13/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.5399 - loss: 1.5026Train Perplexity: 4.53\n",
            "Validation Perplexity: 5.31\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 450ms/step - accuracy: 0.5399 - loss: 1.5026 - val_accuracy: 0.5043 - val_loss: 1.6699\n",
            "Epoch 14/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - accuracy: 0.5413 - loss: 1.5044Train Perplexity: 4.51\n",
            "Validation Perplexity: 5.30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 444ms/step - accuracy: 0.5413 - loss: 1.5044 - val_accuracy: 0.5070 - val_loss: 1.6672\n",
            "Epoch 15/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.5437 - loss: 1.4939Train Perplexity: 4.48\n",
            "Validation Perplexity: 5.33\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 450ms/step - accuracy: 0.5437 - loss: 1.4939 - val_accuracy: 0.5070 - val_loss: 1.6732\n",
            "Epoch 16/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.5458 - loss: 1.4880Train Perplexity: 4.46\n",
            "Validation Perplexity: 5.29\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 448ms/step - accuracy: 0.5458 - loss: 1.4880 - val_accuracy: 0.5096 - val_loss: 1.6653\n",
            "Epoch 17/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 0.5465 - loss: 1.4816Train Perplexity: 4.44\n",
            "Validation Perplexity: 5.29\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m780s\u001b[0m 435ms/step - accuracy: 0.5465 - loss: 1.4816 - val_accuracy: 0.5091 - val_loss: 1.6656\n",
            "Epoch 18/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step - accuracy: 0.5463 - loss: 1.4815Train Perplexity: 4.41\n",
            "Validation Perplexity: 5.26\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 461ms/step - accuracy: 0.5463 - loss: 1.4815 - val_accuracy: 0.5114 - val_loss: 1.6609\n",
            "Epoch 19/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - accuracy: 0.5480 - loss: 1.4763Train Perplexity: 4.40\n",
            "Validation Perplexity: 5.30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 456ms/step - accuracy: 0.5480 - loss: 1.4763 - val_accuracy: 0.5108 - val_loss: 1.6669\n",
            "Epoch 20/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.5483 - loss: 1.4722Train Perplexity: 4.39\n",
            "Validation Perplexity: 5.31\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m788s\u001b[0m 448ms/step - accuracy: 0.5483 - loss: 1.4722 - val_accuracy: 0.5078 - val_loss: 1.6691\n",
            "Epoch 21/30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 0.5505 - loss: 1.4654Train Perplexity: 4.37\n",
            "Validation Perplexity: 5.30\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m823s\u001b[0m 460ms/step - accuracy: 0.5505 - loss: 1.4654 - val_accuracy: 0.5101 - val_loss: 1.6685\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c4afa9385d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ],
      "metadata": {
        "id": "cbvbBOp3MfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, gen_length=300, temperature=1.0):\n",
        "    model_input = [char2id.get(c, char2id[' ']) for c in seed_text.lower()]\n",
        "    input_seq = model_input[-seq_length:]\n",
        "\n",
        "    generated = seed_text\n",
        "    for _ in range(gen_length):\n",
        "        pad_input = np.array([input_seq[-seq_length:]])\n",
        "        preds = model.predict(pad_input, verbose=0)[0]\n",
        "\n",
        "        preds = np.asarray(preds).astype(\"float64\")\n",
        "        preds = np.log(preds + 1e-9) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        next_id = np.random.choice(len(preds), p=preds)\n",
        "        next_char = id2char[next_id]\n",
        "\n",
        "        generated += next_char\n",
        "        input_seq.append(next_id)\n",
        "\n",
        "    return generated\n"
      ],
      "metadata": {
        "id": "1uHjn6aHMW5K"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 1\n",
        "print(\"Sample 1: 'once upon a time'\")\n",
        "print(generate_text(model, \"once upon a time\", gen_length=300, temperature=0.8))\n",
        "\n",
        "# Sample 2\n",
        "print(\"\\n Sample 2: 'The world was'\")\n",
        "print(generate_text(model, \"the world was\", gen_length=300, temperature=0.8))\n"
      ],
      "metadata": {
        "id": "n5CpdqF9MoPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b88e01f-7552-4759-8479-7be43700ad58"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: 'once upon a time'\n",
            "once upon a time i had on another one she said by a little girl of his flast the barnowly confident could contended from a\n",
            "greats anyhe asked at the\n",
            "truet up that they was with all way after the bown conversed came and just which she spoke and he was bening surpy mothed the room. is had sense. she was comperman and\n",
            "\n",
            " Sample 2: 'The world was'\n",
            "the world was holassed tom. ashes the esterge and or the longer on mys of the money. i had no back and the some wellspets betcented back a group that the rook of unfilleg his voice he said i think in the afternoon\n",
            "of clampare on my bendain faster as if he see it\n",
            "was from conversias in it as he demanded turning\n",
            "r\n"
          ]
        }
      ]
    }
  ]
}