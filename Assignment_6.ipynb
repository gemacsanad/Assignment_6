{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ],
      "metadata": {
        "id": "CtuSrazlNYEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ],
      "metadata": {
        "id": "vriXNd_nL2q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Combine all sonnets into a single text source.  \n",
        "- Split into training (80%) and validation (20%).  "
      ],
      "metadata": {
        "id": "Q5atve1sMH9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O book.txt https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
        "\n",
        "with open(\"book.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Loaded text length:\", len(text))\n",
        "\n",
        "split_index = int(len(text) * 0.8)\n",
        "train_text = text[:split_index]\n",
        "test_text = text[split_index:]\n",
        "\n",
        "print(\"Train text length:\", len(train_text))\n",
        "print(\"Test text length:\", len(test_text))"
      ],
      "metadata": {
        "id": "QvKdt5EyMDug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fec19d5-f983-4608-dce6-0b3c7247e5e8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-20 15:11:54--  https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 306594 (299K) [text/plain]\n",
            "Saving to: ‘book.txt’\n",
            "\n",
            "book.txt            100%[===================>] 299.41K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-20 15:11:54 (2.79 MB/s) - ‘book.txt’ saved [306594/306594]\n",
            "\n",
            "Loaded text length: 290077\n",
            "Train text length: 232061\n",
            "Test text length: 58016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ],
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def preprocess(raw_text):\n",
        "    raw_text = raw_text.lower()\n",
        "    allowed_punctuation = ['.', '!', '?']\n",
        "    clean_text = \"\"\n",
        "    for c in raw_text:\n",
        "        if c.isalnum() or c.isspace() or c in allowed_punctuation:\n",
        "            clean_text += c\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "train_text = preprocess(train_text)\n",
        "test_text = preprocess(test_text)\n",
        "\n",
        "\n",
        "chars = sorted(set(train_text))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "char2id = {ch: idx for idx, ch in enumerate(chars)}\n",
        "id2char = {idx: ch for ch, idx in char2id.items()}\n",
        "\n",
        "train_ids = [char2id[c] for c in train_text]\n",
        "test_ids = [char2id.get(c, char2id[' ']) for c in test_text]\n",
        "\n",
        "seq_length = 100\n",
        "\n",
        "def create_sequences(token_ids, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(token_ids) - seq_length):\n",
        "        X.append(token_ids[i:i + seq_length])\n",
        "        y.append(token_ids[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_train, y_train = create_sequences(train_ids, seq_length)\n",
        "X_test, y_test = create_sequences(test_ids, seq_length)\n",
        "\n",
        "print(\"Char vocab size:\", vocab_size)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "RvXRFVcbMLe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d91323d5-8127-430e-d0fa-ed904d9cadde"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char vocab size: 46\n",
            "X_train shape: (223547, 100)\n",
            "y_train shape: (223547,)\n",
            "X_test shape: (56059, 100)\n",
            "y_test shape: (56059,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ],
      "metadata": {
        "id": "jbTZs3OiMMNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=256, input_length=20),\n",
        "    GRU(128),\n",
        "    Dropout(0.2),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "OXCK40l6MRld"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ],
      "metadata": {
        "id": "qsXR4RZpMXMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "linweGaUMg0T"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible."
      ],
      "metadata": {
        "id": "Ggop4h4IMhMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "class PerplexityCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        train_loss = logs.get('loss')\n",
        "        val_loss = logs.get('val_loss')\n",
        "        if train_loss:\n",
        "            print(f\"Train Perplexity: {math.exp(train_loss):.2f}\")\n",
        "        if val_loss:\n",
        "            print(f\"Validation Perplexity: {math.exp(val_loss):.2f}\")\n",
        "\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[PerplexityCallback(), early_stop]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "P8d8FS2XMj46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219d5817-c973-4453-d27f-c7a6965c775f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - accuracy: 0.3108 - loss: 2.3897Train Perplexity: 8.55\n",
            "Validation Perplexity: 7.17\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m813s\u001b[0m 463ms/step - accuracy: 0.3108 - loss: 2.3895 - val_accuracy: 0.4181 - val_loss: 1.9700\n",
            "Epoch 2/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.4393 - loss: 1.8655Train Perplexity: 6.26\n",
            "Validation Perplexity: 6.32\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m838s\u001b[0m 450ms/step - accuracy: 0.4393 - loss: 1.8655 - val_accuracy: 0.4594 - val_loss: 1.8438\n",
            "Epoch 3/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.4714 - loss: 1.7471Train Perplexity: 5.66\n",
            "Validation Perplexity: 5.99\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m802s\u001b[0m 450ms/step - accuracy: 0.4714 - loss: 1.7471 - val_accuracy: 0.4737 - val_loss: 1.7908\n",
            "Epoch 4/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - accuracy: 0.4903 - loss: 1.6799Train Perplexity: 5.33\n",
            "Validation Perplexity: 5.78\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m802s\u001b[0m 449ms/step - accuracy: 0.4903 - loss: 1.6799 - val_accuracy: 0.4811 - val_loss: 1.7538\n",
            "Epoch 5/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 0.5041 - loss: 1.6338Train Perplexity: 5.14\n",
            "Validation Perplexity: 5.61\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m788s\u001b[0m 451ms/step - accuracy: 0.5041 - loss: 1.6338 - val_accuracy: 0.4902 - val_loss: 1.7250\n",
            "Epoch 6/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - accuracy: 0.5125 - loss: 1.6070Train Perplexity: 4.99\n",
            "Validation Perplexity: 5.53\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 438ms/step - accuracy: 0.5125 - loss: 1.6070 - val_accuracy: 0.4958 - val_loss: 1.7094\n",
            "Epoch 7/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - accuracy: 0.5206 - loss: 1.5778Train Perplexity: 4.88\n",
            "Validation Perplexity: 5.49\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 436ms/step - accuracy: 0.5206 - loss: 1.5778 - val_accuracy: 0.4992 - val_loss: 1.7036\n",
            "Epoch 8/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - accuracy: 0.5260 - loss: 1.5616Train Perplexity: 4.78\n",
            "Validation Perplexity: 5.41\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 434ms/step - accuracy: 0.5260 - loss: 1.5616 - val_accuracy: 0.5008 - val_loss: 1.6886\n",
            "Epoch 9/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - accuracy: 0.5280 - loss: 1.5484Train Perplexity: 4.71\n",
            "Validation Perplexity: 5.43\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 453ms/step - accuracy: 0.5280 - loss: 1.5484 - val_accuracy: 0.5013 - val_loss: 1.6917\n",
            "Epoch 10/10\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - accuracy: 0.5323 - loss: 1.5340Train Perplexity: 4.66\n",
            "Validation Perplexity: 5.37\n",
            "\u001b[1m1747/1747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m810s\u001b[0m 457ms/step - accuracy: 0.5323 - loss: 1.5340 - val_accuracy: 0.5058 - val_loss: 1.6816\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c4a551490d0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ],
      "metadata": {
        "id": "cbvbBOp3MfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, gen_length=300, temperature=1.0):\n",
        "    model_input = [char2id.get(c, char2id[' ']) for c in seed_text.lower()]\n",
        "    input_seq = model_input[-seq_length:]\n",
        "\n",
        "    generated = seed_text\n",
        "    for _ in range(gen_length):\n",
        "        pad_input = np.array([input_seq[-seq_length:]])\n",
        "        preds = model.predict(pad_input, verbose=0)[0]\n",
        "\n",
        "        preds = np.asarray(preds).astype(\"float64\")\n",
        "        preds = np.log(preds + 1e-9) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        next_id = np.random.choice(len(preds), p=preds)\n",
        "        next_char = id2char[next_id]\n",
        "\n",
        "        generated += next_char\n",
        "        input_seq.append(next_id)\n",
        "\n",
        "    return generated\n"
      ],
      "metadata": {
        "id": "1uHjn6aHMW5K"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 1\n",
        "print(\"Sample 1: 'once upon a time'\")\n",
        "print(generate_text(model, \"once upon a time\", gen_length=300, temperature=0.8))\n",
        "\n",
        "# Sample 2\n",
        "print(\"\\n Sample 2: 'The world was'\")\n",
        "print(generate_text(model, \"the world was\", gen_length=300, temperature=0.8))\n"
      ],
      "metadata": {
        "id": "n5CpdqF9MoPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56bbe23-9fe9-47db-edac-2ced83ec4ca9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: 'once upon a time'\n",
            "once upon a timent the some about the first i told me that they like she fowly fine body one spitared pived with a moment to cneek his\n",
            "jairy more was lift about you togen the sprontor and there was i mohe moune in his timengly sand. it was mister come and sund years and certainy on the precainting jordan producentl\n",
            "\n",
            " Sample 2: 'The world was'\n",
            "the world was a led to tan mostept in a in the rible time that we were and i was first rooms well they were to withone had dir. you had were and shister and from the rase which lied troubling vitimently of the warted him and from new if who horder he jumpeless said biloxpace she said voice stepuring the man sudd\n"
          ]
        }
      ]
    }
  ]
}